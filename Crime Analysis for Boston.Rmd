---
title: "Crime Analysis for Boston"
output: html_document
date: "2023-08-27"
---

# WHAT IS THE PROBLEM ABOUT

This problem involves the Boston data set. Here, I am predicting per capita crime rate using the some of the variables in this data set. In simple words, per capita crime rate is the response, and the other variables are the predictors.

I HAVE DIVIDED THE PROJECT INTO SECTIONS SO THAT IT GIVES CLARITY TO THE VIEWER ON WHAT TO EXPECT IN EACH STAGE

# OVERVIEW OF THE FIRST STEPS TAKEN

First, for each predictor, I will fit a simple linear regression model to predict the response. Then you can find my take on the results below. I will also mention in which of the models there is a statistically significant association between the predictor and the response. What will this do? Well, it will help us understand to choose the best model in similar datasets in the future with similar circumstances. Then finally, I will also create some plots to back up your assertions.

```{r}
library(MASS)
data("Boston")
colnames(Boston)
lm.fit1<- lm(crim ~ zn, Boston)
summary(lm.fit1)
plot(Boston$zn, Boston$crim, pch = 20, main = "Relationship of zn and crim")
plot(lm.fit1)
```
#Here, we see that the pvalue is low so we can reject null Hyposthesis and determine that there is a significant relationship between zn and crim.


```{r}
library(MASS)
data("Boston")
colnames(Boston)
lm.fit2<- lm(crim ~ indus, Boston)
summary(lm.fit2)
plot(Boston$indus, Boston$crim, pch = 20, main = "Relationship of indus and crim")
plot(lm.fit2)
```
#Here, we see that the pvalue is low so we can reject null Hyposthesis and determine that there is a significant relationship between indus and crim.


```{r}
library(MASS)
data("Boston")
colnames(Boston)
lm.fit3<- lm(crim ~ chas, Boston)
summary(lm.fit3)
plot(Boston$chas, Boston$crim, pch = 20, main = "Relationship of chas and crim")
plot(lm.fit3)
```
#Here, p-value is greater than 0.05 so we cannot reject null hypothesis. Thus, there is no significant relationship between chas and crim


```{r}
library(MASS)
data("Boston")
colnames(Boston)
lm.fit4<- lm(crim ~ nox, Boston)
summary(lm.fit4)
plot(Boston$nox, Boston$crim, pch = 20, main = "Relationship of nox and crim")
plot(lm.fit4)
```
#Here, we see that the pvalue is low so we can reject null Hyposthesis and determine that there is a significant relationship between nox and crim.


```{r}
library(MASS)
data("Boston")
colnames(Boston)
lm.fit5<- lm(crim ~ rm, Boston)
summary(lm.fit5)
plot(Boston$rm, Boston$crim, pch = 20, main = "Relationship of rm and crim")
plot(lm.fit5)
```
#Here, p value was low so null hypothesis could be rejected, but R squared value and adjusted R squared value is also low so relationship between rm and crim is not significant.


```{r}
library(MASS)
data("Boston")
colnames(Boston)
lm.fit6<- lm(crim ~ age, Boston)
summary(lm.fit6)
plot(Boston$age, Boston$crim, pch = 20, main = "Relationship of age and crim")
plot(lm.fit6)
```
#Here, p value was low so null hypothesis could be rejected, but R squared value and adjusted R squared value is also low so relationship between age and crim is not significant.


```{r}
library(MASS)
data("Boston")
colnames(Boston)
lm.fit7<- lm(crim ~ dis, Boston)
summary(lm.fit7)
plot(Boston$dis, Boston$crim, pch = 20, main = "Relationship of dis and crim")
plot(lm.fit7)
```
#Here, p value was low so null hypothesis could be rejected, but R squared value and adjusted R squared value is also low so relationship between dis and crim is not significant.


```{r}
library(MASS)
data("Boston")
colnames(Boston)
lm.fit8<- lm(crim ~ rad, Boston)
summary(lm.fit8)
plot(Boston$rad, Boston$crim, pch = 20, main = "Relationship of rad and crim")
plot(lm.fit8)
```
#Here, p value was low so null hypothesis could be rejected, but R squared value and adjusted R squared value is also low so relationship between rad and crim is not significant.


```{r}
library(MASS)
data("Boston")
colnames(Boston)
lm.fit9<- lm(crim ~ tax, Boston)
summary(lm.fit9)
plot(Boston$tax, Boston$crim, pch = 20, main = "Relationship of tax and crim")
plot(lm.fit9)
```
#Here, p value was low so null hypothesis could be rejected, but R squared value and adjusted R squared value is also low so relationship between tax and crim is not significant.


```{r}
library(MASS)
data("Boston")
colnames(Boston)
lm.fit10<- lm(crim ~ ptratio, Boston)
summary(lm.fit10)
plot(Boston$ptratio, Boston$crim, pch = 20, main = "Relationship of ptratio and crim")
plot(lm.fit10)
```
#Here, p value was low so null hypothesis could be rejected, but R squared value and adjusted R squared value is also low so relationship between ptratio and crim is not significant.


```{r}
library(MASS)
data("Boston")
colnames(Boston)
lm.fit11<- lm(crim ~ black, Boston)
summary(lm.fit11)
plot(Boston$black, Boston$crim, pch = 20, main = "Relationship of black and crim")
plot(lm.fit11)
```
#Here, p value was low so null hypothesis could be rejected, but R squared value and adjusted R squared value is also low so relationship between rm and crim is not significant.


```{r}
library(MASS)
data("Boston")
colnames(Boston)
lm.fit12<- lm(crim ~ lstat, Boston)
summary(lm.fit12)
plot(Boston$lstat, Boston$crim, pch = 20, main = "Relationship of lstat and crim")
plot(lm.fit12)
```
#Here, p value was low so null hypothesis could be rejected, but R squared value and adjusted R squared value is also low so relationship between lstat and crim is not significant.


```{r}
library(MASS)
data("Boston")
colnames(Boston)
lm.fit13<- lm(crim ~ medv, Boston)
summary(lm.fit13)
plot(Boston$medv, Boston$crim, pch = 20, main = "Relationship of medv and crim")
plot(lm.fit13)
```
#Here, p value was low so null hypothesis could be rejected, but R squared value and adjusted R squared value is also low so relationship between medv and crim is not significant.



###############################


# OVERVIEW OF THE NEXT STEPS TAKEN

I will fit a multiple regression model to predict the response using all of the predictors and then I will also write a small explanation of the results. Finally, I will determine for which predictors can we reject the null hypothesis H0 : βj = 0? What will it do, well it will help us determines which variables are important predictors.

```{r}

lm.fitmultiple <- lm(crim~.,data = Boston)
summary(lm.fitmultiple)
```
#we see that the predictors "zn", "dis", "rad", "black" and "medv" are statistically significant becuase of their low p-value. Hence we can reject the null-hypothesis for these predictors.




##########################################

# OVERVIEW OF THE THIRD PART

Then comparing how the results from (a) compare to the results from (b)? Then I will create a plot displaying the univariate regression coefficients from (a) on the x-axis, and the multiple regression coefficients from (b) on the y-axis. That is, each predictor will be displayed as a single point in the plot. Its coefficient in a simple linear regression model will be shown on the x-axis, and its coefficient estimate in the multiple linear regression model will be shown on the y-axis.
```{r}
#First lets create a vector with the coefficients of all the simple regression models in a.
simplecoef <- vector("numeric", 0)
simplecoef <- c(simplecoef, lm.fit1$coefficients[2])
simplecoef <- c(simplecoef, lm.fit2$coefficients[2])
simplecoef <- c(simplecoef, lm.fit3$coefficients[2])
simplecoef <- c(simplecoef, lm.fit4$coefficients[2])
simplecoef <- c(simplecoef, lm.fit5$coefficients[2])
simplecoef <- c(simplecoef, lm.fit6$coefficients[2])
simplecoef <- c(simplecoef, lm.fit7$coefficients[2])
simplecoef <- c(simplecoef, lm.fit8$coefficients[2])
simplecoef <- c(simplecoef, lm.fit9$coefficients[2])
simplecoef <- c(simplecoef, lm.fit10$coefficients[2])
simplecoef <- c(simplecoef, lm.fit11$coefficients[2])
simplecoef <- c(simplecoef, lm.fit12$coefficients[2])
simplecoef <- c(simplecoef, lm.fit13$coefficients[2])
simplecoef

#Now, creating a vector for the multiple regression coefficients
multiplecoef <- vector("numeric", 0)
multiplecoef <- c(multiplecoef, lm.fitmultiple$coefficients)
multiplecoef <- multiplecoef[-1]
multiplecoef

#Now plotting,
plot(simplecoef, multiplecoef, col = "green", pch = 20, ylab = "Coefficients of multiple regression", xlab = "Coefficients of Univariate regression", main = "Plot between Multiple regression coefficients and Univariate regression coefficients")
```
#Now explaining the first part of the question, we see that the coefficients in (a) or Univariate regression coefficients and the coefficients in (b) or multiple regression coefficients have a striking difference. We see that the according to multiple regression, per capita crime has almost no relationship with the a lot of the predictors if not all. However, in the simple regression it is not the case as there are association of per capita crime with a lot of predictors.





##################################


# OVERVIEW OF THE NEXT PART

# I am checking if there is evidence of non-linear association between any of the predictors and the response? To answer this question, for each predictor X, I will fit a model of the form Y = β0 + β1X + β2X2 + β3X3 + ϵ.
```{r}
#crim=β0+β1(zn)+β2(zn)2+β3(zn)3+ϵ
lm.zn <- lm(crim ~ zn + I(zn^2) + I(zn^3), data = Boston)
summary(lm.zn)
```
#Zn and crim does not have a non-linear association as the p-values for the degree 2 term and the degree three term are large.



```{r}
#crim=β0+β1(indus)+β2(indus)2+β3(indus)3+ϵ
lmindus <- lm(crim ~ indus + I(indus^2) + I(indus^3), data = Boston)
summary(lmindus)
```
#Since the p values of both the degree 2 term and degree3 term are small, there is a non-linear association with crim.



```{r}
#crim=β0+β1(chas)+β2(chas)2+β3(chas)3+ϵ
lmchas <- lm(crim ~ chas + I(chas^2) + I(chas^3), data = Boston)
summary(lmchas)
```
#it shows NA as chas is a factor so it does not affect the crime rate.



```{r}
#crim=β0+β1(nox)+β2(nox)2+β3(nox)3+ϵ
lmnox <- lm(crim ~ nox + I(nox^2) + I(nox^3), data = Boston)
summary(lmnox)
```
#as per p-values, nox does have a non-linear association with crim



```{r}
#crim=β0+β1(rm)+β2(rm)2+β3(rm)3+ϵ
lmrm <- lm(crim ~ rm + I(rm^2) + I(rm^3), data = Boston)
summary(lmrm)
```
#as per p-value, rm does NOT have a non-linear association with crim



```{r}
#crim=β0+β1(age)+β2(age)2+β3(age)3+ϵ
lmage <- lm(crim ~ age + I(age^2) + I(age^3), data = Boston)
summary(lmage)
```
#as per  p-values, age has a non-linear association with crim



```{r}
#crim=β0+β1(dis)+β2(dis)2+β3(dis)3+ϵ
lmdis <- lm(crim ~ dis + I(dis^2) + I(dis^3), data = Boston)
summary(lmdis)
```
#as per the p values, dis has a non-linear association with crim



```{r}
#crim=β0+β1(rad)+β2(rad)2+β3(rad)3+ϵ
lmrad <- lm(crim ~ rad + I(rad^2) + I(rad^3), data = Boston)
summary(lmrad)
```
#as per the p values, rad does not have a non-linear association with crim



```{r}
#crim=β0+β1(tax)+β2(tax)2+β3(tax)3+ϵ
lmtax <- lm(crim ~ tax + I(tax^2) + I(tax^3), data = Boston)
summary(lmtax)
```
#as per p value, tax does not have a non-linear association with crim



```{r}
#crim=β0+β1(ptratio)+β2(ptratio)2+β3(ptratio)3+ϵ
lmptratio <- lm(crim ~ ptratio + I(ptratio^2) + I(ptratio^3), data = Boston)
summary(lmptratio)
```
#as per p value, ptratio shows a non-linear association with crim



```{r}
#crim=β0+β1(black)+β2(black)2+β3(black)3+ϵ
lmblack <- lm(crim ~ black + I(black^2) + I(black^3), data = Boston)
summary(lmblack)
```
#as per p values, black does not have a non-linear association with crim



```{r}
#crim=β0+β1(lstat)+β2(lstat)2+β3(lstat)3+ϵ
lmlstat <- lm(crim ~ lstat + I(lstat^2) + I(lstat^3), data = Boston)
summary(lmlstat)
```
#as per the p-value, lstat does not have a non-linear association with crim



```{r}
#crim=β0+β1(medv)+β2(medv)2+β3(medv)3+ϵ
lmmedv <- lm(crim ~ medv + I(medv^2) + I(medv^3), data = Boston)
summary(lmmedv)
```
#based on p-values, medv has a non-linear association with crim.



# OVERVIEW OF THE NEXT SECTION

We will now try to predict per capita crime rate.

For that we will try out some of the regression methods, such as best subset selection, the lasso, ridge regression, and PCR. Then we will present and discuss results for the approaches that we consider.

#1) subset selection - exhaustive
```{r}
library(MASS)
library(glmnet) 
library(leaps)
data(Boston)
head(Boston)
set.seed(123)


indis <- sample(1:nrow(Boston), round(2/3*nrow(Boston)), replace = FALSE)
boston_train <- Boston[indis, ]
boston_test <- Boston[-indis, ]


#using exhaustive selection method
regfit.full <- regsubsets(crim~., data = boston_train, nbest = 1, nvmax = 13, method = "exhaustive")
my_sum <- summary(regfit.full)


par(mfrow=c(2,2))
plot(my_sum$rss,xlab="Number of Variables",ylab="RSS",type="l")

plot(my_sum$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
p = which.max(my_sum$adjr2)
points(p,my_sum$adjr2[p], col="red",cex=2,pch=20)

plot(my_sum$cp,xlab="Number of Variables",ylab="Cp",type='l')
p = which.min(my_sum$cp)
points(p,my_sum$cp[p],col="red",cex=2,pch=20)

plot(my_sum$bic,xlab="Number of Variables",ylab="BIC",type='l')
p = which.min(my_sum$bic)
points(p,my_sum$bic[p],col="red",cex=2,pch=20)

#identifying the optimal models
which(my_sum$cp == min(my_sum$cp))
which(my_sum$bic == min(my_sum$bic))
which(my_sum$rss == min(my_sum$rss))
which(my_sum$adjr2 == max(my_sum$adjr2))
```

#subset selection - forward
```{r}
regfit.fwd <- regsubsets(crim~., data = boston_train, nbest = 1, nvmax = 13, method = "forward")

summary_fwd <- summary(regfit.fwd)


# examining the best "p" variables models
summary_fwd$outmat

par(mfrow=c(2,2))
plot(summary_fwd$rss,xlab="Number of Variables",ylab="RSS",type="l")

plot(summary_fwd$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
p = which.max(summary_fwd$adjr2)
points(p,summary_fwd$adjr2[p], col="red",cex=2,pch=20)

plot(summary_fwd$cp,xlab="Number of Variables",ylab="Cp",type='l')
p = which.min(summary_fwd$cp)
points(p,summary_fwd$cp[p],col="red",cex=2,pch=20)

plot(summary_fwd$bic,xlab="Number of Variables",ylab="BIC",type='l')
p = which.min(summary_fwd$bic)
points(p,summary_fwd$bic[p],col="red",cex=2,pch=20)

#identifying the optimal models
which(summary_fwd$cp == min(summary_fwd$cp))
which(summary_fwd$bic == min(summary_fwd$bic))
which(summary_fwd$rss == min(summary_fwd$rss))
which(summary_fwd$adjr2 == max(summary_fwd$adjr2))
```

#subset selection - backward
```{r}
regfit.bwd <- regsubsets(medv~., data = boston_train, nbest = 1, nvmax = 13, method = "backward")

summary_bwd <- summary(regfit.bwd)

# examine the best "p" variables models
summary_bwd$outmat


Test_error <- mean((mean(boston_test$crim)-boston_test$crim)^2)

par(mfrow=c(2,2))
plot(summary_bwd$rss,xlab="Number of Variables",ylab="RSS",type="l")

plot(summary_bwd$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
p = which.max(summary_bwd$adjr2)
points(p,summary_bwd$adjr2[p], col="red",cex=2,pch=20)

plot(summary_bwd$cp,xlab="Number of Variables",ylab="Cp",type='l')
p = which.min(summary_bwd$cp)
points(p,summary_bwd$cp[p],col="red",cex=2,pch=20)

plot(summary_bwd$bic,xlab="Number of Variables",ylab="BIC",type='l')
p = which.min(summary_bwd$bic)
points(p,summary_bwd$bic[p],col="red",cex=2,pch=20)

#identifying the optimal models
which(summary_bwd$cp == min(summary_bwd$cp))
which(summary_bwd$bic == min(summary_bwd$bic))
which(summary_bwd$rss == min(summary_bwd$rss))
which(summary_bwd$adjr2 == max(summary_bwd$adjr2))
coef(regfit.full, 13)
coef(regfit.fwd, 13)
coef(regfit.bwd, 13)
```



#Linear regression
```{r}
lm.fit <- lm(crim~., data = boston_train)
lm_pred <- predict(lm.fit, boston_test )
Test_error_linear <- mean((boston_test$crim - lm_pred)^2)
Test_error_linear

#plotting the same using the best subset from exhaustive, forward and backward which are the optiamal models as the predictors now
lm.fit1 = lm(crim~medv+dis+indus+black+ptratio, data=boston_train)
lm_pred1 <- predict(lm.fit1, boston_test )
Test_error1_linear_with_bestsubset <- mean((boston_test$crim - lm_pred1)^2)
Test_error1_linear_with_bestsubset
```
#Performing LASSO
```{r}
set.seed(123)
X_train = model.matrix(crim~., data = boston_train)
X_test = model.matrix(crim~., data = boston_test)
#Choosing lambda using cross-validation
cv.out = cv.glmnet(X_train, boston_train$crim, alpha=1)
sel = cv.out$lambda.min
sel
lasso_mod = glmnet(X_train, boston_train$crim, alpha=1, lambda=sel)
#Make predictions
lasso_pred = predict(lasso_mod, s=sel, newx=X_test)
Test_error_lasso <- mean((lasso_pred - boston_test$crim)^2)
Test_error_lasso
```
#RIDGE
```{r}
cv.out = cv.glmnet(X_train, boston_train$crim, alpha=0)
sel2 = cv.out$lambda.min
sel2
ridge_mod = glmnet(X_train, boston_train$crim,alpha = 0)
#Make predictions
ridge_pred = predict(ridge_mod,s=sel2, newx =X_test, lambda=sel2)
#Calculate test error
Test_error_ridge <- mean((ridge_pred - boston_test$crim)^2)
Test_error_ridge
```


#PCR
```{r}
library(pls)
set.seed(123)

pcrfit <- pcr(crim~., data=boston_train, scale=TRUE, validation="CV")

validationplot(pcrfit, val.type = "MSEP")
summary(pcrfit)

#From the summary and the plot, the lowest MSEP occur at M = 13.
pcrfit1 <- pcr(crim~., data=boston_train, scale=TRUE, ncomp=13)
prediction <- predict(pcrfit1, boston_test, ncomp=13)

#test error
Test_error_pcr <- mean((prediction-boston_test$crim)^2)
Test_error_pcr
```
#also confirmed the M value by changing the value of ncomp value from 2 to 3 and 8 to 13 and got the minimum value of test error at 13. Hence, considered M = 13 in the final answer.

#PLS
```{r}
set.seed(123)
#Fit and determine M based on CV results
plsfit = plsr(crim~., data=boston_train, scale=TRUE, validation="CV")
validationplot(plsfit, val.type = "MSEP")
summary(plsfit)

#From the summary and the plot, the lowest MSEP occur at M = 13.
plsfit1 = plsr(crim~., data=boston_train, scale=TRUE, ncomp=13)
prediction = predict(plsfit1, boston_test, ncomp = 13)
#test error
Test_error_pls <- mean((prediction - boston_test$crim)^2)
Test_error_pls
```
#also confirmed the M value by changing the value of ncomp value from 2 to 3 and 8 to 13 and got the minimum value of test error at 13. Hence, considered M = 13 in the final answer.

#Now to discuss the results of the approaches that I performed
```{r}
Test_error
Test_error_linear
Test_error1_linear_with_bestsubset
Test_error_lasso
Test_error_ridge
Test_error_pcr
Test_error_pls
```
#Comapring the results, we see that the linear model, Pcr and pls model perform similarly producing the same test error. The linear model here, performs a little better with all the predictors then with the best selected predictors. Overall, the ridge model here performs slightly better. However, the difference is negligible.



# Now lets propose a model (or set of models) that seem to perform well on this data set, and justify my answer. 

Here, I am Making sure that I am evaluating model performance using validation set error, crossvalidation, or some other reasonable alternative, as opposed to using training error.

So, I will propose Ridge model as it has a slightly lower test error than other models and as it also stabilizes the model and prevents overcomplicating. However, even Lasso can be considered depending on the requirement and context as it also produces low test error. But since here no context or requiremnet is mentioned, I am proposing Ridge based on its lowest test_error values observed. Hence the best model to predict college applications is Ridge. Following it will be the Lasso model. This evaluation has been made by performing cross-validation and validation set error as that is how the test error values were obtained.



# Again, does my chosen model involve all of the features in the data set, let see

Well Ridge Regression, the chosen model, does use all the features (predictors) in the dataset. Ridge Regression uses a technique called L2 regularization, which penalizes the model based on the square of the coefficients. Unlike Lasso, Ridge doesn't force coefficients to become zero aggressively. Instead, it reduces their impact, helping to stabilize the model and avoid overfitting. Ridge usually includes all the features in the model but manages their influence to create a more balanced and reliable prediction preventing any single feature from dominating the prediction. This ensures a more stable and less prone to overfitting model. It is more about finding the right balance between the best predictors.



#######################################################